The genesis of this project was frustration that the smarts of PlexAmp were going to waste because it wasn't able to use effectively the ratings that I had provided for my library. It seemed obvious that if I've rated just a few tracks on an album, then Plex should treat the album as if it had been rated at the average of those tracks, and similarly for artists with respect to albums. Even more obviously, if an album has a rating, and one of it's tracks doesn't then Plex simply guessing that the track has the same rating as the album it's on will be reasonably accurate.

I implemented that logic, and lived with the results for awhile. While I found that it really did improve the experience, there were some aspects that bothered me, and a lot of thinking and experimenting led to further refinements. I'll try here to explain how I've tried to augment and finesse the data for greater accuracy.

The first realization was that Bayes tells us that we need account for our lack of knowledge. While knowing the rating for a few tracks on an album tells us a bunch, we can't ignore the fact that there's a broader world out there, and there's a good chance that some of that album will be at least a little like the rest of the world rather than the rated tracks. The `CONFIDENCE_C` factor was added to account for this. The operation is to pretend that there are this many additional tracks in the album, each having a rating given by the global mean rating.

While working on that, I also realized that we've got for some albums an additional piece of data that was being completely ignored: critics' ratings for albums. After normalizing that to Plex's internal 10-point rating scale, it was obvious that we'd need to have a balance of how to weight this against other evidence for an album's rating. Thus was born `WEIGHT_CRITIC`. Once that was in place, it quickly became obvious that critics don't score the same way that I do. It seems like in critic language, a low score tends to mean "not up to the artist's previous standards" and similar, rather than "bad album". To adjust for this I added `BIAS_CRITIC`, and 1.5 seemed a good value. This operates by adding the value to the critic's score and then normalizing back to the 10-point scale. So if a critic rated it at 60%, I'd convert that to 6.0. Then calculate 

`normalized_critic_rating = (6.0 + 1.5) / (10 + 1.5) = 6.5` (or 3.25 stars) 

By now you might be getting the impression that I'm fooling myself with the amount of precision I'm pretending to be working with. At least, I got that impression myself. Combine that with the fact that I've got a large library, and running this tool took a considerable period of time. I can sacrifice a little bit of precision for improved performance. The addition of `DYNAMIC_PRECISION` was to save time updating data (it's the storage of the update that takes so long). If the size of the calculated update would be smaller than the precision target, we just won't do the update this time. Since this is primarily driven by performance, it made sense to allow for greater precision in smaller libraries, and loosen things up as the library grows. It does this logarithmically. 

The next addition was Gravity. There are precious few albums, even of the greatest, that don't have a couple of tracks that only bridge between the amazing songs without contributing much themselves. So it shouldn't be expected that every one of a 5.0 album's tracks are 5.0 themselves. And even more so, ratings of Artists are more about "these are my favorite artists" than about an expectation that every one of my favorite artist's albums, and every one of their songs, are 5's. The app now has a "gravity" computation that does reversion to the mean of albums within an artist, and tracks within an album, according to adjustable weights. Those settings (`ALBUM_INHERITANCE_GRAVITY` and `TRACK_INHERITANCE_GRAVITY`) signify a weighting factor that pulls the rating of an Artist and of an Album towards the global mean (whether that's upwards or backwards, it's always reversion toward the mean) when pushing inherited ratings to their unrated children.

Up to this point I'd been concentrating on "vertical" computations: aggregating the scores of children to apply to their parent, and inheriting parent scores to apply to their children. But there are more relationships between items that we can apply to discover even more information. 

I've got a lot of tracks in my library that appear more than once - maybe even a half-dozen times - on their original recording, on greatest hits and other compilations, live albums, and so forth. If I can find one of those that has a manual rating, I can confidently assign that same ratings to its twins. Even if I can't find a manual rating, I can better refine the inferred rating by taking the mean of all inferences of its twin scores. So this adds a "horizontal" component of inference.

The actual math in handling twins is pretty trivial. But there are hidden pitfalls that make the *logic* a lot more complicated. A lot of tracks are twins in name only - consider the recordings of *Dazed and Confused* as they appear on *Led Zeppelin I* and *The Song Remains the Same*. The former clocks in at 6:28, but the latter is a full album side extended jam at 26:52. These are only superficially the same song, and shouldn't share a rating. And even if they were of similar lengths, my experience is that live recordings are almost always sufficiently different that they shouldn't get the same rating. 

This turned into an exercise of finding meaningful heuristics to differentiate superficially-similar songs. Length was the first factor, from which grew `DURATION_TOLERANCE_SEC`. But I also discovered that Plex's idea of what's a live recording (what they call "subformat") helps, and also looking for parentheses (which frequently denote things like "2025 remix"), as well as the presence of keywords like "live", "remix", and "instrumental" can be a tip-off. 

Having figured that out, it seemed that I could logically apply it back to the original calculations I'd already been doing. For example, in live recordings there are often interstitial tracks with names like "intro" or in some compilations, "interview". It's common for Plex users to down-rate these so that Plex avoids them. We shouldn't penalize the albums for this. So a similar suite of rules about keywords and minimum durations was added to avoid unduly tainting album rating inference.

And that brings us to where we are today.


